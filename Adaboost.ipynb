{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c89d6ff",
   "metadata": {},
   "source": [
    "# Adaboost (Adaptive Boosting)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec5e7f1",
   "metadata": {},
   "source": [
    "Adaboost combines multiple weak learners into a single strong learner. \n",
    "Unlike Random forest, Adaboost combones weeker learners (Decision Trees in a sequential manner)\n",
    "The decision trees (DT) in adaboost are single split/one depth on nature and are called decision stumps (DS)\n",
    "To develop a single base learner, it first compares information gain of each DT based on each of the feature\n",
    "and selects the DT with information gain/entrophy/gini impurities. This becomes the week learner.\n",
    "This method does not follow Bootstrapping. \n",
    "The number of decision stumps it will make will depend on the number of features in the dataset. Suppose there are M features then, Adaboost will create M  decision stumps. \n",
    "Following are the steps in Adaboost:\n",
    "1. A new matrix called sample weight will be used to assign weight to each observation. for M number of features, initial weight will be 1/M.\n",
    "2. To generate the first base learner/week learner (BS), M decision stumps are generated from M number of features. Based on their information gain, best DS is selected. \n",
    "3. From this DS, total error (TE) is calculated based on misclassifition of samples by that DS. If  total misclassification is T, TE=T/N where N is number of samples.\n",
    "4. Based on TE, its performace score(PS) is calculated, PS= 1/2*log(base e)((1-TE)/TE) \n",
    "5. Based on PS, new weights will be assigned to samples which are classified correctly and incorrectly.\n",
    "6. New weight for incorrectly classified sample: old weight * (e**(PS))\n",
    "7. New weight for correctly classified sample: old weight * (e**(-PS))\n",
    "8. This will increase the weight of incorrectly classified samples and decrease the weight of correctly classified samples. Which means that the next BS clasifier will have to give more importance in learning the incorrectly classified samples.\n",
    "9. If the summation of the new weights are =! 1, we need to normalize the weight as : (new weight)/ summation of (all new weights)\n",
    "10. Based on new weights, some buckets/ranges/classes of normalized weights are formed.  These weights will be used to form the new sample set for classification be the next weak learner.\n",
    "11. Based on some iterations for N numbber of times, and psudo randomly generated numbers between (0-1) the new samples are selected from the old sample list based on where it falls in the buckets of normalized weights.\n",
    "12. The process between step (2-11) are repeated till the error reduces to the minimal.\n",
    "11. During the testing of data, each data will be classified using the multiple BS, and a majority voting  will be used to generate the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1065cb64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
